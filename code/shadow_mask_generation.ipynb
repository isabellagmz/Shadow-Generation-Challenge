{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f7c081a",
   "metadata": {},
   "source": [
    "# Shadow Mask Generator\n",
    "Isabella Gomez\n",
    "\n",
    "This is the implementation of the first half the Shadow Generation for Composite Image in Real-World Scenes paper (https://arxiv.org/pdf/2104.10338.pdf), which is a Shadow Mask Generator.\n",
    "\n",
    "I will be using the DESOBA dataset which was created for this project by the authors. \n",
    "\n",
    "Steps:\n",
    "1. Downloading dataset \n",
    "2. Creating Train/Test pairs\n",
    "3. Create Ebs and Efs\n",
    "4. Create CAI\n",
    "5. Create Decoder\n",
    "6. Build U-net model\n",
    "7. Initialize loss functions and optimizers\n",
    "8. Train the model\n",
    "9. Show some of the output images\n",
    "\n",
    "After this runs, I will have a saved trained model for the Shadow Predition Stage and I will have saved all the predicted shadow masks from training and testing. These will then be used in the Shadow area filling stage which will be done in another notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a87a8e",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Initialize cuda and import libraries. In this project we will be using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d37954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.init as init\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# setup\n",
    "device = None\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba649d4",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "\n",
    "Make a custom dataset and load in my data.\n",
    "\n",
    "Q: Should I eliminate the data that is not Ic, Mfos, Mbos in the transformation? i.e. only keep the relevant information for Ebs, Efs or should I just keep all of it? \n",
    "\n",
    "A: None yet, for now I am keeping all of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5960a619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11509\n",
      "581\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.imgs = os.listdir(root_dir)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.imgs[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Convert to color\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "transform = T.Compose([T.ToTensor()]) # can add more if necessary\n",
    "    \n",
    "# Batch size during training\n",
    "batch_size = 1\n",
    "\n",
    "path_train = 'DESOBA_DATASET/TrainTestVisualization/train'\n",
    "path_test = 'DESOBA_DATASET/TrainTestVisualization/test_bos'\n",
    "dataset = CustomDataset(path_train, transform=transform)\n",
    "dataset_size = dataset.__len__()\n",
    "dataset_test = CustomDataset(path_test, transform=transform)\n",
    "dataset_test_size = dataset_test.__len__()\n",
    "\n",
    "# Create the dataloaders\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size)\n",
    "\n",
    "# size of my training set\n",
    "print(dataset_size)\n",
    "print(dataset_test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318b3fea",
   "metadata": {},
   "source": [
    "### Initializing Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35aa50ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on encoders, to test various weight initializations\n",
    "\n",
    "def weights_init(net, init_type='normal', gain=0.02):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if init_type == 'normal':\n",
    "                init.normal_(m.weight.data, 0.0, gain)\n",
    "            elif init_type == 'xavier':\n",
    "                init.xavier_normal_(m.weight.data, gain=gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                init.orthogonal_(m.weight.data, gain=gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('BatchNorm2d') != -1:\n",
    "            init.normal_(m.weight.data, 1.0, gain)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    #print('initialize network with %s' % init_type)\n",
    "    net.apply(init_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebdce4c",
   "metadata": {},
   "source": [
    "# Part 1: Creating Background and Foreground Encoders\n",
    "\n",
    "Here I will first be getting Ic, Mfos, and Mbos (composite image, Forground Object Mask, Background Object Mask) in order to design the foreground and background encoders (Efs/Ebs respectively). These will be designed as per the paper instructions as follows:\n",
    "\n",
    "**Ebs / Efs**\n",
    "- DBlk layer, with AvgPool, 128×128×64\n",
    "- DBlk layer, with AvgPool, 64×64×128\n",
    "- DBlk layer, with AvgPool, 32×32×256\n",
    "- DBlk layer, with AvgPool, 16×16×512\n",
    "\n",
    "In this case DBlk means Downsampling Block. The design of these encoders is based off of the U-Net architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d7f942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared downsampling block\n",
    "class DBlk(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.avg_pool(x)\n",
    "        return x\n",
    "\n",
    "# Foreground encoder (parameter sharing)\n",
    "class EncoderFs(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # input is 256x256x32\n",
    "        # Shared downsampling blocks\n",
    "        self.down_block1 = DBlk(in_channels=32, out_channels=64)\n",
    "        self.down_block2 = DBlk(in_channels=64, out_channels=128)\n",
    "        self.down_block3 = DBlk(in_channels=128, out_channels=256)\n",
    "        self.down_block4 = DBlk(in_channels=256, out_channels=512)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skip1 = self.down_block1(x)\n",
    "        skip2 = self.down_block2(skip1)\n",
    "        skip3 = self.down_block3(skip2)\n",
    "        x = self.down_block4(skip3)\n",
    "        \n",
    "        \n",
    "        return x, skip1, skip2, skip3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b6650ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# background encoder (no parameter sharing)\n",
    "class EncoderBs(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # input is 256x256x32\n",
    "        # downsampling block 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.avg_pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        # downsampling block 2\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.avg_pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        # downsampling block 3\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.avg_pool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        # downsampling block 4\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        self.avg_pool4 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Save intermediate feature maps for skip connections\n",
    "        skip1 = self.conv1(x)\n",
    "        skip1 = self.bn1(skip1)\n",
    "        x = self.relu1(skip1)\n",
    "        x = self.avg_pool1(x)\n",
    "\n",
    "        skip2 = self.conv2(x)\n",
    "        skip2 = self.bn2(skip2)\n",
    "        x = self.relu2(skip2)\n",
    "        x = self.avg_pool2(x)\n",
    "\n",
    "        skip3 = self.conv3(x)\n",
    "        skip3 = self.bn3(skip3)\n",
    "        x = self.relu3(skip3)\n",
    "        x = self.avg_pool3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.avg_pool4(x)\n",
    "        \n",
    "        return x, skip1, skip2, skip3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a7570",
   "metadata": {},
   "source": [
    "# Part 2: Creating CAI (Cross-Attention Integration) Layer\n",
    "\n",
    "The Cross-Attention Integration layer is a layer that will help the foreground feature map Xf attend relevant illumination information from background feature map Xb.\n",
    "\n",
    "The cross-attention mechanism takes as input the foreground and background encoder features, Xf and Xb respectively, and computes the attended background features using the foreground features. \n",
    "\n",
    "A change that was made between my implementation and the paper is that the query was the one transposed instead of the key in the matmul. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a70d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAI(nn.Module):\n",
    "    def __init__(self, in_channels = 512):\n",
    "        super().__init__()\n",
    "        # Define cross-attention mechanism components\n",
    "        self.query_conv = nn.Conv2d(in_channels, in_channels//8, kernel_size=1)\n",
    "        self.query_conv = nn.utils.spectral_norm(self.query_conv)\n",
    "        \n",
    "        self.key_conv = nn.Conv2d(in_channels, in_channels//8, kernel_size=1)\n",
    "        self.key_conv = nn.utils.spectral_norm(self.key_conv)\n",
    "        \n",
    "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.value_conv = nn.utils.spectral_norm(self.value_conv)\n",
    "        \n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.conv = nn.Conv2d(in_channels*2, in_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, Xf, Xb):\n",
    "        Nf, Cf, Hf, Wf = Xf.shape\n",
    "        Nb, Cb, Hb, Wb = Xb.shape\n",
    "        \n",
    "        # Apply cross-attention mechanism to attend to background_features\n",
    "        query = self.query_conv(Xf)\n",
    "        key = self.key_conv(Xb)\n",
    "        value = self.value_conv(Xb) \n",
    "        \n",
    "        # reshape\n",
    "        query = query.reshape(Nf, Cf//8, Hf*Wf)\n",
    "        key = key.reshape(Nb, Cb//8, Hb*Wb)\n",
    "        value = value.reshape(Nb, Cb, Hb*Wb)\n",
    "\n",
    "        # calculate the affinity_map\n",
    "        A = torch.matmul(query.transpose(1,2), key)\n",
    "        A = torch.softmax(A, dim=-1)\n",
    "        \n",
    "        attention = torch.matmul(value, A)\n",
    "        \n",
    "        # Reshape the output to (N, C, H, W)\n",
    "        Xb_prime = attention.reshape(Nb, Cb, Hb, Wb)\n",
    "        \n",
    "        output = torch.cat([Xb_prime*self.gamma, Xf], dim=1)\n",
    "        output = self.conv(output)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51266022",
   "metadata": {},
   "source": [
    "# Part 3: Creating Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4f95672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared upsampling block\n",
    "class UBlk(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up_conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv = nn.Conv2d(out_channels+out_channels, out_channels, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        x = self.up_conv(x)\n",
    "        \n",
    "        if skip!=None: # is not first layer\n",
    "            \n",
    "            # Calculate the difference in spatial dimensions between x and skip\n",
    "            diff_h = x.size(2) - skip.size(2)\n",
    "            diff_w = x.size(3) - skip.size(3)\n",
    "            # Pad x to match the spatial dimensions of skip\n",
    "            skip = F.pad(skip, (diff_w // 2, diff_w - diff_w // 2, diff_h // 2, diff_h - diff_h // 2))\n",
    "            \n",
    "            x = torch.cat([x,skip], dim=1)\n",
    "            x = self.conv(x) # fix channel dimensions\n",
    "        \n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# Decoder\n",
    "class Decoder(nn.Module): # Ds\n",
    "    def __init__(self):\n",
    "        super().__init__() # input is 16×16×512\n",
    "        # Shared upsampling blocks\n",
    "        self.up_block1 = UBlk(in_channels=512, out_channels=512)\n",
    "        self.up_block2 = UBlk(in_channels=512, out_channels=256)\n",
    "        self.up_block3 = UBlk(in_channels=256, out_channels=128)\n",
    "        self.up_block4 = UBlk(in_channels=128, out_channels=64)        \n",
    "        \n",
    "    def forward(self, x, skip1, skip2, skip3):\n",
    "        x = self.up_block1(x)  # No skip connection in the first layer\n",
    "        x = self.up_block2(x, skip3)\n",
    "        x = self.up_block3(x, skip2)\n",
    "        x = self.up_block4(x, skip1) \n",
    "        \n",
    "        return x     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e18a050",
   "metadata": {},
   "source": [
    "# Part 4: Building the U-Net Model\n",
    "\n",
    "In this part we build the U-net model that is used in the first half of the procedure. This is the Shadow Mask Prediction Stage where the final output is a mask of the foreground object shadow that we can then compare to the ground truth.\n",
    "\n",
    "Then we will initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82b7ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # first convolution to set up input\n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=3, padding=1)\n",
    "        \n",
    "        ### Encoder\n",
    "        self.efs = EncoderFs().to(device)\n",
    "        self.ebs = EncoderBs().to(device)\n",
    "        \n",
    "        ## Cross-Attention Integration (CAI)\n",
    "        self.attention = CAI()\n",
    "        \n",
    "        ### Decoder\n",
    "        self.d = Decoder().to(device)\n",
    "        \n",
    "        # last convolution to finish output\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1)\n",
    "    \n",
    "    def forward(self, Mfs, Mbs): \n",
    "        ''' \n",
    "        Mbs - concatenation of Ic and Mbos meant for Ebs\n",
    "        Mfs - concatenation of Ic and Mfo meant for Efs\n",
    "        '''\n",
    "        Mfs = self.conv1(Mfs)\n",
    "        Mbs = self.conv1(Mbs)\n",
    "        \n",
    "        Efs, skip1, skip2, skip3 = self.efs(Mfs) # foreground encoder\n",
    "        Ebs, skipb1, skipb2, skipb3 = self.ebs(Mbs) # background encoder\n",
    "        attention = self.attention(Efs, Ebs) # CAI\n",
    "        Ds = self.d(attention, skip1, skip2, skip3) # decoder\n",
    "        \n",
    "        output = self.conv2(Ds) # foreground object shadow mask\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6285317f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# testing model for output size\n",
    "data = torch.randn((1,4,256,256)).to(device)\n",
    "u = Unet().to(device)\n",
    "out = u(data, data)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1cd8c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet(\n",
      "  (conv1): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (efs): EncoderFs(\n",
      "    (down_block1): DBlk(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (avg_pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (down_block2): DBlk(\n",
      "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (avg_pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (down_block3): DBlk(\n",
      "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (avg_pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (down_block4): DBlk(\n",
      "      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (avg_pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "  )\n",
      "  (ebs): EncoderBs(\n",
      "    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU(inplace=True)\n",
      "    (avg_pool1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU(inplace=True)\n",
      "    (avg_pool2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu3): ReLU(inplace=True)\n",
      "    (avg_pool3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu4): ReLU(inplace=True)\n",
      "    (avg_pool4): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  )\n",
      "  (attention): CAI(\n",
      "    (query_conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (key_conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (value_conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (d): Decoder(\n",
      "    (up_block1): UBlk(\n",
      "      (up_conv): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (up_block2): UBlk(\n",
      "      (up_conv): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (up_block3): UBlk(\n",
      "      (up_conv): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (up_block4): UBlk(\n",
      "      (up_conv): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (conv2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# initializing model\n",
    "unet = Unet().to(device)\n",
    "unet.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595131b8",
   "metadata": {},
   "source": [
    "# Part 5: Preparing the Input\n",
    "\n",
    "This step is necessary to separate the Ic, Mbos, Mfo from the training/testing image pairs. The input to the foreground encoder needs the composite image Ic, and the foreground object mask Mfo. The input to the background encoder is Ic, and the background object and shadow mask Mbos. \n",
    "\n",
    "In order to get Ic and Mbos, Mfo, we need to consider the length of the training image itself. Each image section is 256x256 pixels in length with a total training image size of 1536 pixels. This means that there are 6 sections in each training image. The first 256x256 pixels correspond to Ic, the second section corresponds to the original image, the third is Mfo, the fourth corresponds to the mask of the foreground object shadow as ground truth, the fifth is Mbos, and the sixth is the mask of the background object shadow.\n",
    "\n",
    "In the input_sectioning function we will be extracting all the parts from each training image to use individually in the training loop. The foreground Encoder requires Mfo and Ic, and the background encoder requires Mbos and Ic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca07b256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input management\n",
    "def input_sectioning(whole_image):\n",
    "    original_image = whole_image[:, 256:512]\n",
    "    Ic = whole_image[:, 0:256] # composite image\n",
    "    Mfo = whole_image[:, 512:768] # foreground object mask\n",
    "    Mfo_shadow = whole_image[:, 768:1024] # foreground object shadow mask for ground truth    \n",
    "    \n",
    "    Mbo = whole_image[:, 1024:1280] # background object mask\n",
    "    Mbo_shadow = whole_image[:, 1280:1536] # background object shadow mask\n",
    "    Mbos = Mbo + Mbo_shadow # background object-shadow pair\n",
    "    \n",
    "    # changing the images to be 256x256x1\n",
    "    # Take the maximum along the channel dimension to collapse it to a single channel\n",
    "    Mbos = torch.max(Mbos, dim=2)[0]\n",
    "    Mfo = torch.max(Mfo, dim=2)[0]\n",
    "    Mfo_shadow = torch.max(Mfo_shadow, dim=2)[0]\n",
    "\n",
    "    # Reshape the tensor to have the shape 256x256x1\n",
    "    Mbos = Mbos.unsqueeze(dim=2)\n",
    "    Mfo = Mfo.unsqueeze(dim=2)\n",
    "    Mfo_shadow = Mfo_shadow.unsqueeze(dim=2)\n",
    "\n",
    "    return original_image, Ic, Mfo, Mbos, Mfo_shadow\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaea76f",
   "metadata": {},
   "source": [
    "# Part 6: Loss Function and Optimizer\n",
    "\n",
    "Here I will initialize the optimizer, in this case it is Adam. I will also be defining the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9a92d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for tuning\n",
    "beta1 = 0.5\n",
    "beta2 = 0.99\n",
    "lr = 0.0002\n",
    "epochs = 1\n",
    "\n",
    "# trained model path\n",
    "model_path = 'models/unet.pth'\n",
    "\n",
    "# shadow mask folder\n",
    "mask_path_train = 'DESOBA_DATASET/Mfs_train'\n",
    "mask_path_test = 'DESOBA_DATASET/Mfs_test'\n",
    "\n",
    "\n",
    "# Initialize the ``BCELoss`` function\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Setup Adam optimizers\n",
    "optimizer = optim.Adam(unet.parameters(), lr=lr, betas=(beta1, beta2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c1dc48",
   "metadata": {},
   "source": [
    "# Part 7: Training Loop\n",
    "\n",
    "The paper has some confusing statements regarding the input to Efs, Ebs. First it claims the input is the concatenation of Ic and Mfo, or Ic and Mbos respectively. However, later on it claims the input is a concatenation of Ic, Mfo, Mbos of dimension 256x256x5 which is then passed into a convolutional layer that makes it 256x256x32. \n",
    "            \n",
    "What I am going to do is to make it 256x256x4 and use the concatenation as it was first stated in the methodology and pass it through the convolutional layer to change its dimension to 1x32x256x256 where Batch size=1, Channels=32, H=256, W=256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5bda4d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_loop(model, optimizer, dataloader, dataloader_test, epochs=1, device=device):\n",
    "    model = model.to(device)  # move the model parameters to CPU/GPU\n",
    "    \n",
    "    max_score = 100\n",
    "    total_train_steps = len(dataloader)\n",
    "    total_test_steps = len(dataloader_test)\n",
    "    \n",
    "    # Start the training cycle\n",
    "    print('Start training...')\n",
    "    \n",
    "    # train loop\n",
    "    for e in range(epochs):\n",
    "        model.train()  # put model to training mode\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # For each batch in the dataloader\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            \n",
    "            if i == 1000: # for efficiency, we will limit to 1k images\n",
    "                break\n",
    "                \n",
    "            training_object_pair = data[0].permute(1, 2, 0)\n",
    "            original_image, Ic, Mfo, Mbos, Mfo_shadow = input_sectioning(training_object_pair)\n",
    "            \n",
    "            # concatenate Ic and Mfo and Ic and Mbos\n",
    "            Mfs = torch.cat([Ic, Mfo], dim=2).to(device) # 256x256x4\n",
    "            Mbs = torch.cat([Ic, Mbos], dim=2).to(device)\n",
    "            \n",
    "            # reshape into 1x4x256x256\n",
    "            Mfs = torch.unsqueeze(Mfs.permute(2, 0, 1), 0) # 1x4x256x256\n",
    "            Mbs = torch.unsqueeze(Mbs.permute(2, 0, 1), 0)\n",
    "            Mfo_shadow = torch.unsqueeze(Mfo_shadow.permute(2, 0, 1), 0)\n",
    "            \n",
    "            # run through model\n",
    "            model_output = model(Mfs, Mbs)\n",
    "            \n",
    "            # save model output masks on the last epoch\n",
    "            if e == epochs-1:\n",
    "                # Get the 2D shadow mask from the model_output and move it to CPU\n",
    "                shadow_mask = model_output.squeeze(0).squeeze(0).cpu()\n",
    "\n",
    "                # Convert the shadow mask to a binary array using a threshold of -2.5\n",
    "                binary_mask = (shadow_mask >= -2).numpy().astype('uint8')\n",
    "                \n",
    "                # Save the binary mask as an image\n",
    "                mask_image = Image.fromarray(binary_mask * 255, mode='L')\n",
    "                mask_image.save(f'DESOBA_DATASET/Mfs_train/{i:04d}.png')\n",
    "            \n",
    "            # Compare the model output with ground truth data\n",
    "            ground_truth = Mfo_shadow.to(device)\n",
    "            loss_result = loss(model_output, ground_truth)            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_result.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Output training stats\n",
    "            if i % 100 == 0:\n",
    "                print('[%d/%d]\\tTraining Loss: %.4f'\n",
    "                      % (e, epochs, loss_result.item()))\n",
    "            \n",
    "            train_loss += loss_result.item()\n",
    "            \n",
    "        # Put the model in eval mode\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        \n",
    "        # test loop\n",
    "        for i, data in enumerate(dataloader_test, 0):\n",
    "            testing_object_pair = data[0].permute(1, 2, 0)\n",
    "            original_image, Ic, Mfo, Mbos, Mfo_shadow = input_sectioning(testing_object_pair)\n",
    "        \n",
    "            # concatenate Ic and Mfo and Ic and Mbos\n",
    "            Mfs = torch.cat([Ic, Mfo], dim=2).to(device) # 256x256x4\n",
    "            Mbs = torch.cat([Ic, Mbos], dim=2).to(device)\n",
    "        \n",
    "            # reshape into 1x4x256x256\n",
    "            Mfs = torch.unsqueeze(Mfs.permute(2, 0, 1), 0) # 1x4x256x256\n",
    "            Mbs = torch.unsqueeze(Mbs.permute(2, 0, 1), 0)\n",
    "            Mfo_shadow = torch.unsqueeze(Mfo_shadow.permute(2, 0, 1), 0)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                model_output = model(Mfs, Mbs)\n",
    "            \n",
    "            # save model output masks on the last epoch\n",
    "            if e == epochs-1:\n",
    "                # Get the 2D shadow mask from the model_output and move it to CPU\n",
    "                shadow_mask = model_output.squeeze(0).squeeze(0).cpu()\n",
    "\n",
    "                # Convert the shadow mask to a binary array using a threshold of 0.5\n",
    "                binary_mask = (shadow_mask >= -2).numpy().astype('uint8')\n",
    "\n",
    "                # Save the binary mask as an image\n",
    "                mask_image = Image.fromarray(binary_mask * 255, mode='L')\n",
    "                mask_image.save(f'DESOBA_DATASET/Mfs_test/{i:04d}.png')\n",
    "            \n",
    "            # Compare the model output with ground truth data\n",
    "            ground_truth = Mfo_shadow.to(device)\n",
    "            loss_result = loss(model_output, ground_truth)\n",
    "            \n",
    "             # Output testing stats\n",
    "            if i % 50 == 0:\n",
    "                print('[%d/%d]\\tTesting Loss: %.4f'\n",
    "                      % (e, epochs, loss_result.item()))\n",
    "            \n",
    "            test_loss += loss_result.item()\n",
    "              \n",
    "        train_loss = train_loss / total_train_steps\n",
    "        test_loss = test_loss / total_test_steps\n",
    "        \n",
    "        print(f'\\nEpoch {e}, train_loss: {train_loss}, test_loss: {test_loss}')\n",
    "        \n",
    "        # Save last epoch model\n",
    "        if e == epochs-1:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print('Model saved!')\n",
    "        \n",
    "    return test_loss, train_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "353ad046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[0/1]\tTraining Loss: 0.0203\n",
      "[0/1]\tTraining Loss: 0.0116\n",
      "[0/1]\tTraining Loss: 0.0183\n",
      "[0/1]\tTraining Loss: 0.0150\n",
      "[0/1]\tTraining Loss: 0.0116\n",
      "[0/1]\tTraining Loss: 0.1801\n",
      "[0/1]\tTraining Loss: 0.0402\n",
      "[0/1]\tTraining Loss: 0.1530\n",
      "[0/1]\tTraining Loss: 0.0273\n",
      "[0/1]\tTraining Loss: 0.0161\n",
      "[0/1]\tTesting Loss: 0.0056\n",
      "[0/1]\tTesting Loss: 0.0042\n",
      "[0/1]\tTesting Loss: 0.0115\n",
      "[0/1]\tTesting Loss: 0.0107\n",
      "[0/1]\tTesting Loss: 0.0389\n",
      "[0/1]\tTesting Loss: 0.1068\n",
      "[0/1]\tTesting Loss: 0.0282\n",
      "[0/1]\tTesting Loss: 0.0036\n",
      "[0/1]\tTesting Loss: 0.0453\n",
      "[0/1]\tTesting Loss: 0.4806\n",
      "[0/1]\tTesting Loss: 0.0419\n",
      "[0/1]\tTesting Loss: 0.0054\n",
      "\n",
      "Epoch 0, train_loss: 0.0030994659180433536, test_loss: 0.03929174880047526\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "Ls_test, Ls_train = train_loop(unet, optimizer, dataloader, dataloader_test, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcb137b",
   "metadata": {},
   "source": [
    "# Show the New Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e118a4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABXkAAAIdCAYAAABoV35uAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABcK0lEQVR4nO3dd5xU5dk//mt2l116FxEbomJDRSX23k00mqJ5NLG3iI8+MdYYNRoTa+wlaqKxxZIndqPG6IMYewwWEBUhotJ734Vld75/+HN/IgtsmTlnz+77/XrNK8zMOfd9YeDi7GfuuU8un88HAAAAAADZVJJ2AQAAAAAANJ2QFwAAAAAgw4S8AAAAAAAZJuQFAAAAAMgwIS8AAAAAQIYJeQEAAAAAMqxsZW/mcrl8UoUAxZfP53Np17Aq+g60LvoOkDR9B0iavgMkrb6+YyUvAAAAAECGCXkBAAAAADJMyAsAAAAAkGFCXgAAAACADFvpjdcAaJ123nnnlb4/ZcqUGDt2bELVAAAAAM2Ry+dXfINFd1+E1sVdX4mIKCkpiWeeeSb222+/et+fPHlyXH755XHTTTclXBmtkb4DJE3fAZKm7wBJq6/vCHmhDXHxka5BgwbF5ptvXu97jz/+eFRWViZWS0VFRVRVVdX73kMPPRSHH354YrXQuuk7QNL0HSBp+g6QtPr6ju0aABIwaNCguPzyy+PAAw+s9/1f/OIXcd1118XixYsTrgwAAADIOiEvQAL23nvvFQa8ERGXX355dOrUKRYtWhRXXnll1NbWJljd/+8///lPPPHEE6nMDQAAADRNSdoFAPClCy64IC677LK44oorij7X0qVL4+KLL17u9U8++SQeeuihos8PAAAAFI6VvAAtzNlnnx3dunWLk08+uWhz1NTUxGWXXRaTJ09e5vUvvviiaHMCAAAAxeHGa9CGuCFAegYOHBiXXXZZ/OAHP2jwOblci/+/C1ZJ3wGSpu8ASdN3gKTV13ds1wCQgDFjxsR5550X+++/f7zwwgsNOudvf/tbkasCAAAAWgMreaEN8Qlzy7DBBhvEXXfdFbvssssqj33ttddi/vz5sf/++ydQGRSevgMkTd8BkqbvAEmrr+/YkxcgYWPHjo1Zs2Y16Ngdd9wxamtrY+TIkTF+/Pg46KCDilwdAAAAkDVW8kIb4hPmlmP11VePxx9/PLbffvsGn7N06dL44osv4s0334zDDz+8iNVB4eg7QNL0HSBp+g6QNHvyArQQU6dOjQMOOCDee++9Bp9TVlYW6623XvTr16+IlQEAAABZI+QFSMmcOXNi6dKljT5vp512irlz58Ydd9xRhKoAAACArBHyAqRohx12iE8++aRR55SWlkbXrl2jQ4cORaoKAAAAyBIhL0CKqqurY+ONN44JEyY0+twf//jHsXTp0vj1r39dhMoAAACArHDjNWhD3BCgZZs+fXr07t27SeeeffbZce2110ZtbW2Bq4Lm0XeApOk7QNL0HSBp9fUdIS+0IS4+Wr7Zs2dH9+7dm3z+4YcfHo8++mhEfLlKeGU9HpKg7wBJ03eApOk7QNKEvNDGufjIhpkzZ0bPnj2bPc5mm20Wn332WZSUfLkzz/z585s9JjSWvgMkTd8BkqbvAEmrr++UpVEIACvWq1evqK2tjVyuedeKPXr0iOeeey7WXnvtiIhYc801Y8mSJfUeW1NTE7Nnz27WfAAAAEA6rOSFNsQnzNnx6aefRv/+/ROb7+OPP4799tsvKisrY9q0aYnNS+un7wBJ03eApOk7QNLq6zslaRQCwMqtt9568eGHHyY230YbbRTjx4+PRx55JDbddNNYY401EpsbAAAAaB4reaEN8QlztpSVlUV1dXUqcz/xxBNxxRVXxOeffx6TJk1KpQZaB30HSJq+AyRN3wGSZiUvAA1y8MEHx+uvvx5HH3102qUAAAAAqyDkBWihamtr43//93/jiSeeSK2GzTffPA499NC6m7cBAAAALY/tGqAN8TWibOrevXvMnj071Rr+8Ic/xJtvvhlPP/10TJ06NdVayBZ9B0iavgMkTd8BklZf3xHyQhvi4iObWkLI+5Vbb701xo8fH3fccUfMnTs37XLIAH0HSJq+AyRN3wGSVl/fKUujEACyaejQoRERsc4668ScOXPiN7/5TSxevDjlqgAAAKBtsycvQAtXWVkZ559/fsHHraqqivPOO69J5/73f/93XHDBBXHzzTfHLbfcErlci1+8AAAAAK2WlbwALdzixYvj+uuvj+rq6rj66qubPM6xxx4bS5curXteXV0dTzzxREycOHGV5w4dOjR22GGH5V4/4YQTIiKia9eukc/n46ijjmpyfQAAAEDT2JMX2hB7RWVbx44d44QTTogbbrihUecdccQRMW/evHjmmWdiZT1/ZQYPHhxrrrlmXHXVVbHpppuu8Linn346qqqq4tBDD23SPLQ++g6QNH0HSJq+AyTNjdegjXPxkX2dO3eOrbfeOnbaaae47LLLVnn8j370o3jssceiurq6IPNvueWW0a1bt3jwwQejX79+9R6zYMGC6NKlS0HmI/v0HSBp+g6QNH0HSJqQF9o4Fx+tR9euXWPAgAEREXHiiSfW3RDt677//e/H008/XbCA9+vGjRtXN/83CXn5On0HSJq+AyRN3wGSJuSFNs7FR+vUo0eP6NWrV0RE3HzzzfHQQw/FK6+8Ep9//nksWbKkKHOuu+668fbbb0fv3r0jImLjjTeOmpqaiIjI5/Mxbty4osxL9ug7QNL0HSBp+g6QNCEvtHEuPlq/7t27x6JFi4oW7n5dnz59orS0NCIiJk+eXPT5yCZ9B0iavgMkTd8BkibkhTbOxQeQNH0HSJq+AyRN3wGSVl/fKUmjEAAAAAAACkPICwAAAACQYUJeAAAAAIAME/ICAAAAAGSYkBcAAAAAIMOEvAAAAAAAGSbkBQAAAADIMCEvAAAAAECGCXkBAAAAADJMyAsAAAAAkGFCXgAAAACADBPyAgAAAABkmJAXAAAAACDDhLwAAAAAABkm5AUAAAAAyDAhLwAAAABAhgl5AQAAAAAyTMgLAAAAAJBhQl4AAAAAgAwT8gIAAAAAZJiQFwAAAAAgw4S8AAAAAAAZJuQFAAAAAMgwIS8AAAAAQIYJeQEAAACggf7zn/9EbW3tMg9Im5AXoJUqLS2NsrKymD59evTo0SMWL14c+Xy+7jF37twoKytb5lFaWpp22QAAAC1SaWlpjB49OtZbb73I5XLLPL7+s9bUqVOX+1mrrKwsSkrEcBSPP10ArdS//vWvmD9/fvTu3TtmzZoV5eXly7zftWvXqK6uXubx7LPPRrt27VKqGAAAoOV68cUXY+ONN17lcX369FnuZ63q6ur4/e9/n0CVtFVCXoBWauutt44OHTrE2LFjG3zOPvvsEw899FB07NixiJUBAABkz+677x5vvvlmk7dnqKioiO7du9c9LLChkHL5fH7Fb+ZyK34TyJx8Pp9Lu4ZV0XeKY8yYMbHhhhs2+Pj7778/zjnnnJg3b14sXLiwiJXR2uk7QNL0HSBp+k7b89Zbb8U222zT7O0XjjjiiHjwwQcLVBVtSX19x0pegDZg4MCBMXr06AYf/5Of/CQmTZoUl1xySWywwQbRuXPnIlYHAACQHdtuu23MmjWr2eP07dvXtygpmEyHvJtttlmUlpbGFltsEYMHD067HIAWbbPNNot33nmnUeeceeaZ8cknn8RBBx1UpKoAAACyZ+TIkU3etuEr1157bRx77LExePDgaN++fYEqo63K9HYNjz76aBx11FHx6KOPRkVFRey2225plwQtmq8RERExfPjwul+3a9cudthhh1We42tENJW+AyRN3wGSpu+0Xc8++2zst99+kcs1/4/AcccdF+PGjYuIiFdffTVqamqaPSatV319J9MhL9A4Lj74pj59+sTUqVNXedzVV18dw4cPj9deey1mz56dQGW0FvoOkDR9B0iavtO2PfbYY3HIIYcUdMwf/ehHsXDhwnjmmWdiZbkdbZeQF9o4Fx98U9euXeOWW26J7t27x4EHHlj3+ieffBJvvvnmMsfuueeeceedd8aYMWPqXnv44Yejuro6sXrJHn0HSJq+AyRN3+Gee+6JkpKS+MlPflLQcY8++uioqamJP//5zwUdl+wT8kIb5+KDFVljjTXiggsuqHv+yiuvLLM9wz777BPXX399bLrppsucd9ZZZ8WiRYvqnufz+bjtttuKXzCZoe8ASdN3gKTpO0RE5HK5+P3vfx8nn3xyQcfN5/Nx6qmnLvf6tGnT4pFHHinoXGSHkBfaOBcfNNWhhx4aW2yxRRx99NGx9tprr/C4mpqauPjii5d5beLEiTFs2LDYZpttXIS0QfoOkDR9B0iavsNXSktL4+KLL45OnTrFGWecUdS5JkyYELfffnvd8zfeeCNeeOGFos5JyyHkhTbOxQfNdfjhh8e1114bffv2bfA5kydPjldeeSW23HLLePLJJ2P48OHx9NNPF7FKWhJ9B0iavgMkTd/hmzp27BgXXnhhnHfeeYnN+fbbb8ewYcPi8ccfj9deey2xeUmHkBfaOBcfFML3vve9uP3222O11VZr0vnXXHNNnHXWWQWuipZK3wGSpu8ASdN3qE+XLl3isMMOW+kxJ510Umy77bYFnfe0006Lm2++uaBj0vIIeaGNc/FBoYwbNy4GDBjQpHPHjBkT7733XkRE3HzzzfHyyy8XsjRaGH0HSJq+AyRN36Gptttuu7jtttti8ODBBRvznXfeibFjx8aVV14ZBx54YNxzzz0xfvz4go1PyyDkhTbOxQeFstNOO0XHjh3j0Ucfjc6dOzd5nFGjRsXkyZMjIuKUU06JcePGFapEWgh9B0iavgMkTd+hOQYPHhz33XdfDBo0qKDjvvvuu7HuuuvGmDFjYt68eXH44YfHzJkzCzoH6RHyQhvn4oNCmzNnTnTr1q0gY40cOTIWLlwYBx10UMyYMaMgY5I+fQdImr4DJE3fobk222yzeOKJJ2L99dcv2hwjRoyIJUuWxO677x6LFy8u2jwkQ8gLbZyLDwpto402itLS0hg5cmSUlJQUZMwxY8bE0qVLY+utt3bx0QroO0DS9B0gafoOhTBgwIBo3759vPzyy9GrV6+izfPRRx9FbW1tbLbZZkWbg+IT8kIb5+KDYll33XUjl8vFp59+GhERs2fPjgMPPDBeffXVJo/5+eefx/rrrx9Lly4tVJmkQN8BkqbvAEnTdyiktdZaK0aPHh1dunQp6jzjx4+P9dZbr6hzUDxCXmjjXHxQbL17946IiHw+H7Nnz46ePXvWvXfQQQfFXXfd1ajxZsyYEbW1tbH66qsXtE6So+8ASdN3gKTpOxRar169Ipdb/o9Vr1694qOPPirYPF9tkzd79uwYOHBgwcal+IS80Ma5+CBNZWVl0aFDh/jv//7vuOyyyxp17vz58yPiy4uPddddtxjlUST6DpA0fQdImr5DUnK5XHTu3Dk22mij+Ne//lXQsefPnx+ffPJJbLPNNgUdl+IQ8kIb5+KDlqCkpCTKysri+uuvj1NOOaVR586aNauo+1NRePoOkDR9B0iavkPScrlctGvXLiIivv/978eDDz5YsLGXLFkSL730Uuy3334FG5PCq6/vFOYuOQDQQLW1tbFkyZI49dRTo7S0NJ5++ukGn9uzZ8+oqampe7zwwgtFrBRoq3K5XFRVVaVdBgBAvfL5fCxZsiSWLFkSDz30UJSWlsbZZ59dkLHLy8tj3333Xebnrl/96lcFGZvispIX2hCfMJMFX3zxRayxxhpRWloaNTU1UVpauspz7r333jjmmGNiZf+mkQ59B0iavgMkTd+hJbnxxhtj6NChDfo5qrGOO+64uPfee6OmpiZKSkqitra24HPQMFbyspzS0tIoLS2NsrKyKCnxxwFI39prrx1lZWUxZ86caN++fVRVVcXixYtXes5RRx0VN954Y1RUVBTlYgYAACALTj/99CgrK4tHH300Fi9evMyjue66665YunRp7L///vHWW2/FFltsERUVFRERdf9LeqzkbePOPPPM6NmzZ/Tu3TtGjBixwn1c5s2bl3BlFINPmMmqsrKymDlzZuRyuejSpctKjz3jjDPi+uuvT6YwVknfAZKm7wBJ03fIivHjxxflRtbrrbdefPDBB9G9e/eorq4u+Pgsr76+U5ZGIbQsp59+epSWlsZJJ50Ut912W8yZMyc6d+4cZWVf/vGYPn16rLPOOvamA1KzdOnS6NatW3Tv3j0+/vjjiPgy+O3Zs2fdMZWVlTF//vxYtGhRWmUCABRFRUVFdOvWbZnX5s2b52c0oFH69+8f48ePjw4dOtS91qdPn2aP++mnnzZ7DJrP9/OJP/7xj/HXv/617vnQoUNjzJgxdc/33XffgizrB2iuOXPmxOqrrx6rr7567LbbbvHpp5/WPS6//PJYffXV44477ki7TACAJispKYl11lkn2rVrF/3794/+/fvHMcccE1OnTl3mcdppp0XHjh3TLhfImP79+9f9TNWvX78YP3582iVRILZraOOOPvroOOuss6Jfv36xaNGimDNnTkREbLDBBvHFF1/UhbtDhgwR9LYCvkYEJE3fAZKm75BluVwudtxxx3jkkUfihBNOiKeeemqlx//sZz+LP/7xj7Fw4cKEKqQ++g5Z1r59+xgxYkRssskmTTr/448/rtuiYauttoqlS5cWsjxWoL6+I+QlIiJuuummmDNnTjz77LMREXH33XfHRRddFJ9//nlERLzxxhvumtgKuPgAkqbvAEnTd8iqXC4Xe+65Z7zwwguNOm/PPfeMYcOGFakqGkLfIeu6du0aL774YgwZMqRBx48aNaru3k3f+973Ytq0acUsj3oIeVmhU045JQ4++OAVvv/d7343lixZkmBFFIOLDyBp+g6QNH2HrCorK2vSDYuEvOnTd2gNevfuHffff3+Djj333HPjvffeK3JFrIyQl5Xabrvt4osvvohJkyZFRMR3vvOd+L//+7+orKxMuTIKxcUHkDR9B0iavkNWNSXkfeWVV+KnP/1pfPDBB0WqiobQd4Ck1dd33HiNOptuumn07t277vl2220X5eXlKVYEAABAfYYPHx6nnXaagBeAiLCSF9oUnzADSdN3gKTpO2TROeecE9dee22jVvIee+yxcffddxevKBpM3wGSVl/fKUujEAAAACDi0ksvjV/+8pfRs2fPlR43YsSI+Otf/1r3/O233y52aQBkiJW80Ib4hBlImr4DJE3fIUtuvPHG+O///u/I5Vb9x/aee+6JY445pvhF0Wj6DpA0N16DNs7FB5A0fQdImr5DlixatCg6dOiw0mNGjx4dv/nNb+LTTz+NN954I6HKaAx9B0ia7RoAAAAgZffdd1/06tUrKioqVnns1KlT48EHH0ygKgCyTMgLAAAACbjrrrtiww03jG233TbKy8vTLgeAVkTICwAAAEV04403xu677x4bbrhhtG/fPu1yAGiFhLwAAABQBL/61a/iqKOOir59+0bHjh0bde6nn34ae++9d1RWVhapOgBaEzdegzbEDQGApOk7QNL0HVqKc845Jy6++OJV3lhtRWpqauLFF1+M/fbbr8CVUWj6DpA0N14DAACABHTu3LlJAe/s2bNjvfXWi4gvg14AaAghLwAAALQQ+Xw+5s6dm3YZAGSM7RqgDfE1IiBp+g6QNH2HliKXy8XNN98cQ4cObdDxS5curbspmxW82aLvAEmrr++UpFEIAAAAtFZnnnlm1NbWNjjgra2tjdra2qipqRHwAtAktmsAAACAFJWWlqZdAgAZJ+QFAACAAlqyZEksWbIkysvLV3rcggULEqoIgNZOyLsCPXv2jJKS5XezqK2tjVmzZqVQEQAAAFlw0003xWqrrRYXXnjhCo+ZNWtWrLnmmlFVVZVgZQC0VkLeFfi///u/2GKLLSKXW3Yf4zlz5kSPHj1SqgoAAICsmzBhQgwaNEjAC0DBuPHaChx22GH+wQUAAKBJpk+fHrNnz673ve222y7mzp2bcEUAtGZW8q7AzTffHBUVFcu8VltbG2+//XZKFQEAAJAVN910U5SXl8ehhx663HtLlixJoSIAWrNcPp9f8Zu53IrfbOV22223+Pvf/75M0Lt48eL4zne+ExERlZWVMXLkyNh2220jImLKlCnxwQcfpFIrNFQ+n8+t+qh0teW+A62RvgMkTd+hpdl0001j2rRpMWPGjLRLoUj0HSBp9fUd2zWswDHHHBNPPfVU/PWvf617raKiIl544YV4+umn4/DDD49evXrFhRdeGPfdd1/sueeeKVYLAABAS7TnnnvGuuuum3YZALRyVvKuxFFHHRWlpaVx1113LfP6nDlz4uc//3lERGy00Uax//77xw033BAffvhhvPHGG2mUCg3iE2YgafoOkDR9B0iavgMkrb6+I+RdicsuuyzKy8tXesxaa60VP/rRjyIi4pVXXonzzjsvXn311STKg0Zz8QEkTd8BkqbvAEnTd4CkCXmLYNddd43hw4fXPf/Nb34Tr7zySkyfPj1GjBiRYmWwPBcfQNL0HSBp+g6QNH0HSFp9facsjUJak7Fjx8bpp58e3/rWt6J///7xt7/9LcrKyqKqqirt0gAAAACANsCN15pp0qRJcdNNN8Xzzz8fffr0iaFDh8ZJJ50Uq622WkHG32677eLYY48tyFgAAAAAQOsj5C2Ql156KYYPHx4VFRXRpUuX2GCDDQoy7oQJE2z7AAAAAACskJC3QCZMmBAjR46M7bbbLoYMGdLkcdZYY42466676p4PHDgw9t5770KUCAAAAAC0Qm68VkCrrbZarL322hER8dlnn8XMmTMbPUZ5eXkMGDAgPvroo4iI6NGjR3Tp0iU+//zzgtZK2+SGAEDS9B0gafoOkDR9B0hafX1HyAttiIsPIGn6DpA0fQdImr4DJK2+vmO7BgAAAACADBPyAgAAAABkmJAXAAAAACDDhLwAAAAAABkm5AUAAAAAyDAhLwAAAABAhgl5AQAAAAAyTMgLAAAAAJBhQl4AAAAAgAwT8gIAAAAAZJiQFwAAAAAgw4S8AAAAAAAZJuQFAAAAAMgwIS8AAAAAQIYJeQEAAAAAMkzICwAAAACQYUJeAAAAAIAME/ICAAAAAGSYkBcAAAAAIMOEvAAAAAAAGSbkBQAAAADIMCEvAAAAAECGCXkBAAAAADJMyAsAAAAAkGFCXgAAAACADBPyAgAAAABkmJAXAAAAACDDhLwAAAAAABkm5AUAAAAAyDAhLwAAAABAhpUlOdnqq68euVwuIiLy+XxMnTo1yekBAAAAAFqdooa8a6+9dpSXl9c9Hz16dN3z6urq2GSTTaKmpibGjx9fzDIAAAAAAFqtXD6fX/GbudyK31yJ/v37x7Rp0+LNN9+MQYMGrfTYGTNmxGqrrdaUaYBGyufzubRrWJWm9h2gZdJ3gKTpO0DS9B0gafX1nYLvybvxxhvHww8/HEcffXR06tRplce3a9cuhgwZUugyAAAAAADahIKu5B00aFDccsstseuuuzaqiGnTpsWhhx4aL7/8cqPOAxrHJ8xA0vQdIGn6DpA0fQdIWn19p2Ah7+DBg+Oaa66JPffcs0nFTZgwIc4777yYNGlSDBs2rEljACvn4gNImr4DJE3fAZKm7wBJq6/vFOzGa/3794++ffs2+fy11lor7r///vjoo4/ixhtvjA8//DBeeumlQpUHAAAAANAqNTvkHTx4cBxwwAEREbFw4cJmF7TxxhvHrbfeGnfeeaeQFwAAAABgFZoV8g4ePDiuvPLK2HfffQtVDwAAAAAAjVDS1BMHDRoU1157bdEC3p133jl++MMfFmVsAAAAAIDWoskh74wZM2LUqFGFrGUZG220UfzmN7+Jhx9+2EphAAAAAIAVaHLIO2XKlHj99dcLWctyNtpoozjssMPi+uuvj7322quocwEAAAAAZFGTQ94kbbLJJnHbbbfFa6+9FoMHD067HAAAAACAFqPJN17beeed46qrripkLSu1wQYbxAYbbBBdu3ZNbE4AAAAAgJauySt5//3vf8fVV19dyFoAAAAAAGikJoW82223Xdx1110xffr0QtcDAAAAAEAjNCnkffvtt+P4448vdC0N8uyzz8a8efNiwIABqcwPAAAAANCS5PL5/IrfzOVW+OZ+++0XTz/9dJSVNXlb32aprq6OfD4fq6++esyZMyeVGiBr8vl8Lu0aVmVlfQfIHn0HSJq+AyRN3wGSVl/fafKevH//+9+joqIiSktL4/zzz29eZU3Qrl27KC8vj5kzZ0aHDh0Snx8AAAAAoCVo8krer5x00knx+9///qvjI5dL5wOsioqKWLJkSSpzQ1b4hBlImr4DJE3fAZKm7wBJK+hK3q/ccccdUVpaGuXl5XHXXXc1d7gmW7x4cVRUVKQ2PwAAAABAGpod8n7lwgsvjKOOOqpQwzVJZWVlqvMDAAAAACSt2ds1fN3tt98eJ510UrOLaqp8Ph8lJQXLraHV8TUiIGn6DpA0fQdImr4DJK0o2zV83YwZM2L8+PGprqjt379/anMDAAAAACStoCt5IyLWWmutePDBB2PnnXduVmFNtXTp0th8883jo48+SmV+aMl8wgwkTd8BkqbvAEnTd4CkFX0lb0TEiSeeGKWlpfH666/HokWLCj38KpWVlcW//vWvxOcFAAAAAEhDwVfyft3IkSNj0KBBzRmiSRYsWBBdunRJfF5o6XzCDCRN3wGSpu8ASdN3gKQlspL365577rn4y1/+EosXLy7mNAAAAAAAbVZZMQd/4403YsyYMXHAAQdERUVFMacCAAAAoBFWX331+O53v1v3/J133om33347xYqApipqyLvOOuvEr371K1snAAAAALQwG264Ydxxxx11z4cNGxa/+MUv4s0330yxKqApirpdw3XXXRezZs0q5hQAAAAANNLqq68eJ5100jKv7bHHHnH55ZfHkCFDUqoKaKqihrwRERdeeGHMnz+/2NMAAAAA0AC9e/eOG2+8MY488sjl3ttjjz1i6623TqEqoDmKHvIOGTIk2rVrV+xpltG+ffu4/fbbE50TAAAAIAu6du0ahx12WNplAAVU9JD3iSeeiMWLFxd7mmUsXbo0/vrXvyY6JwAAAEBL171797jzzjtXesxpp50Wu+66a0IVAYVQ9JD3pZdeSjzkLS8vj9NPPz3ROQEAAABauoqKith9991XesygQYOiX79+yRQEFETRQ96IL/dzWbBgQRJTRURESUlJ7LjjjonNBwAAAACQlrJiTzBs2LBYe+21o2PHjsWeCgAAAACgzSl6yHvYYYdFaWlpRER89tlnUV5eXuwpAQAAAGiic845Jx5//PG0ywAaoejbNUyfPj2mTJkSU6ZMiT59+kRtbW2xp4yIiB49esTIkSMTmQsAAACgtZg7d25UVVWlXQbQCInsyfuVuXPnRqdOnaJDhw71PnbbbbflzrnkkkvqPXbcuHErnWvOnDkxZMiQYv1WAAAAADJn6tSpsdlmm6VdBlBgiYa8ERFVVVXLPVZfffVYsGBBDBs2bLnjL7rooliwYMFyj/XXX3+l8/To0SM++uijYv02AAAAADLJKl1ofRIPeb8pl8vFZ599FoMHD46SkuXLyeVyUVpautyjIcrKyiKXy0Uulyt02QAAAACZ9J///Ce23HLLtMsACijVkLdLly4xceLEWHfddeOdd94p+PhrrbVW1NbWxltvvVXwsQEAAACyqra2NpYuXZp2GUCBpBLydurUKTp16hT5fD423HDDmDlzZuywww5Fm6+kpCTat29ftPEBAAAAsmTUqFGxxx57pF0GUCBlaUy6YMGCmDVrVvTo0SPmzJkTPXr0KOp8W2+9dTzzzDOx5557FnUeAAAAgKyqrKyMRYsW2bMXMiiVlby1tbXxrW99KxYuXJjYp0bt27ePXr16JTIXAAAAQNZcf/310bt377j33nvTLgVopFRW8paUlMQzzzwTnTt3jnfffTeROXfYYYd46KGH4sgjj4wpU6Y0aYyNN954medVVVUxderU6NWrV0yYMKEQZQIAAAAkYtGiRfHRRx/VPZ8+fXqK1QDNkcvn8yt+M5db8ZvNUFNTEzvttFO8/vrrxRh+hebMmRPXXXdd/PrXv270uaWlpfHGG28s89pHH30U11xzTZx22mlx6aWXxvjx4wtUKRRHPp/PpV3DqhSr7wDp0HeApOk7QNL0HSBp9fWdVELefD4fL730Uuy+++7FGH6FHnvssfj+97/frDHKyspil112Wea1HXfcMXbfffe47LLLIiJi3Lhx8fnnnzdrHigGFx9A0vQdIGn6DpA0fQdIWn19J5XtGvL5fNx6662Jh7zNkcvl4pBDDolOnTrFfffdV+8xe++9d0RE3HPPPfHEE0/Em2++GZMmTUqyTAAAmuHggw+OXC4Xjz/+eNqlAABAg6Vy47WIiAMPPDCtqZuktLQ0/vd//zcOOOCAVR579NFHx6OPPho77rhjApUBANAchx12WJSXl8eRRx4Zjz76aDz22GNxzDHHxDHHHBOHH3542uUBAMAqpbKSNyJi5MiRaU3dZLW1tTFq1KgGH/+d73wn1lprrYiIuPfee2PWrFnFKg0AgCa67rrrYv31149LLrkkSkq+XAPxpz/9KSIiZs2aFU899VT84Ac/iGeffTZ22WWXeOSRR9IsFwAAlpNayNu1a9e0pm6ydu3a1e272xDHHHNM3a+ff/55IS8AQAu1smu8bt26xTXXXBM77bRT7LPPPtGuXbt46KGH6t7ff//9Y8qUKfHuu+8mUCkAACwvtZC3R48eic+5xRZbxOGHHx4PPvhg4nP/4he/iJkzZ8Yvf/nLWLhwYeLzAwBkzcknnxybbLJJRERceumlMXPmzNRq6dWrV5x44okREfHb3/42crlcTJw4MTp27Bj77bdfvPfee0JeAABSk8vnV3yDxWLdffHoo4+u+3WXLl3ipptuKsY09br77rvj2GOPbfR5ZWVlUV1d3ez5V1tttZgxY0azx4GmcNdXIGn6Dk1x9NFHxx577BF77bVX3dZXjz76aBx77LExb968gs83ceLE6Nev3zKvHXfccXHXXXfFrFmzYu21147TTjstrrjiioiIGDFiRJx++ukxc+bMqKioiHbt2sWcOXNi7NixBa+NxtN3gKTpO0DS6us7qazkveeee6KsrCz++te/Rnl5eaJz77nnnnHsscfW7bMGAEC6dt555zjrrLPqnm+55ZbRv3//ZY75/ve/Hx07dowf/OAHsWjRooLMe+2118aAAQOiZ8+ey713yCGHRMSXCxIeeOCB6Ny5c917/fr1i7PPPjsiIh566KFltm4AAIA0pLKSNyKivLw8Fi9eXKzhV+i5556L0047rdErLQq1kveVV16JfffdNyorK5s9FjSWT5iBpOk7NESfPn3i3HPPjZ///OerPPaf//xn7LXXXgW5LhsxYkRstdVWzRrjkksuiYsvvrjZtVA4+g6QNH0HSFp9fackjULefvvteP3119OYOr71rW/FQQcd1Ojz3nzzzYLMv/POO8drr70W7dq1K8h4AABZN3jw4PjhD3/YoGN32WWXeOONN+Ltt99u1py33357bLzxxs0a4ytnnHFGk64vAQCgUFIJebfaaqs46qij0pg6hg8fHvfff3+jz/vxj39csBqOPfbYgqw+AQBoDV5//fXYZ599YqONNorbb799hcftsMMOMX/+/Nh6661jm222adacl156aYwfP75ZY3zl/vvvj+HDhxdkLABIysCBA+Pjjz+OP/zhD2mXAhRAKts15PP5mDBhQt2NNJK0aNGiuOqqq+KSSy5p8Dmff/555HK5gtU7adKk6N+/v6CXxPkaEZA0fYfGOOWUU6J79+6x6aabxk9+8pPl3p88eXL07ds3crkv/1hNmDAhIiIWLlzY4FW5119/ffzgBz+IiIjVV1+92d+usl1Dy6PvAEnLQt8pKyvLf/PDzfLy8ujTp08sXrw4pk+fvsx75513Xvz5z39OsEKgMerrO6mEvDU1NdGvX7+YMmVKMYZfpcWLF8dFF10UV111VYOOr62trfthork23HDDmDlzZsyePbsg40FjZOHiww890LroOzRG+/btIyKipKQkKioq4i9/+UvsvffeDTp3ZddWN998c1RWVsbZZ58dnTp1KsiNfx966KEYOnRoVFZWRlVVVbPHo3D0HSBpWeg7ZWVl+aVLlzb4+EWLFtXdR2n//fePt956q1ilAU3QYkLefD4fixYtio4dOxZj+Aaprq6O6urqOOmkk1b46dS0adOiU6dOBa1ztdVWixkzZhRsPGiMLFx8+KEHWhd9h+Z45pln4oADDmj2OEuXLo18Pl/QeyLcc889ccwxxxRsPApH3wGSloW+s2jRonxTs42qqqrYZpttYvTo0QWuCmiqFhPylpWVRXl5eSxcuLAYwzdKbW1t1NbWRkTEdtttFyNGjIhPP/001lprrSgrKyvYPFtuuWU8+eSTMWTIECEvqcnCxYcfeqB10XdojtLS0rpvU3388cex6667xgcffBDdunVLrIZZs2bF6quvvtzrX7+GpGXRd4CkZaHvRESz+k5NTU3k8/lYc801Y9q0aYWqCWii+vpO4VLMRvhqyX9LUFJSEiUlX95/7qu7NBdqa4av+2oFCQAADVNTUxMREe+++24MGDAgvvjii6Jcp9Wnuro6evbsGePHj4/GfL0VAFqj0tLSiIiYMmVK9OjRI+bOnZtyRcA3pbKSN+LLDb5bUthbaCeccELceeedaZcBy8jCJ8xWtkDrou9QKOPGjYsBAwYUZeylS5dGWVlZdOzYMSorK4syB8nRd4CkZaHvRDNX8n5T586dW8S3s6Gtqq/vlKRRSGtXXV1txQcAQAEtWrSo4AHs4sWLo7KyMtZZZ52YNGlSQccGgNZswYIF0blz57TLAL7GSt4iOOuss+Kaa65JuwxYThY+YbayBVoXfYdC6t69e3z22WfRtWvXZo81f/782HHHHWPUqFEFqIyWRN8BkpaFvhMFXsn7lZ49ey7zvLa21lYOkIAWsydvREQ+n49JkyZFaWlpvTezyKoFCxbEggUL0i4DAKDVeemll+qutfr169fkcWbNmhXf+c53BLwAtBmTJ0+ONdZYo+Djzpo1a5nnEyZMiO22267eY+fNmycvgSJKbSXvV3r37h3Tp08v9jSJ+c1vfhMXXnhh2mVAvbLwCbOVLdC66DsUy9KlS+tuAtMYU6ZMiSOOOCKGDRtWhKpoCfQdIGlZ6DtlZWX5tLeVvP766+OWW26JqVOnxvz581OtBbKuRa3k/crSpUvjnXfeqXvetWvX6NWrV8ybNy/WWWedFCtrnNmzZ8f48ePt5wYAkIB33303SkpWfnuJ7t27x3rrrRfTpk2LiRMnRkTEhRdeKOAFoM3J5/PLZC8r0rdv36Ks+I2I+NnPfhY/+9nP4oILLoi//e1vMXbsWCt7oYBSX8n7Tfvss0+cf/758a9//SvOPvvspKdvsvvvvz+OPPLItMuAlcrCJ8xWtkDrou+QlHbt2sU222wTb7zxRkR8GfD+z//8T1x88cVxzTXXxFlnnZVyhSRF3wGS1pr6zo9//OM48cQTY+DAgUULe7/ys5/9LN5999146623Cn5zVWjtWuRK3m+aPn16zJ8/P/bdd9+0S1mpBQsWLLMKZMSIESlWAwDQtrVv3z5OPPHEupB33XXXjW222Saeeuqp+OCDD1KuDgCy4c9//nP8+c9/jlNPPTX222+/utd333336NKlS0Hnuv766yMiYsMNN4yxY8cWdGxoi1rcSt6vHH/88fHHP/4xrelXaezYsbHhhhumXQY0Smv6hBnIBn0HSJq+AyStLfSdkSNHxqBBgwpVzjKEvNB4mVjJ+5UPPvggbr755th+++1jyJAhaZcTEV/uH3zbbbdFRLSqm8UBAAAArMgDDzwQ/fr1i+OPPz46dOhQsHEffvjhmDt3bsHGg7asxa7k/cqll14aF1xwQdplREREZWVldOzYMe0yoMnawifMQMui7wBJ03eApLWlvnPOOedE586d4/zzz4/S0tJmjXXvvffGueeeG1OmTClEadCmZGol71eee+65mDNnThxxxBGx9dZbp1ZHbW1t/OIXv0htfgAAAIA0XXXVVRERMWvWrFWGvAMHDoyTTjqp7vlzzz0XL7zwQt3zhx9+WMALBdTiV/J+Zdddd42bbroptthii1TmP/bYY+Puu+9OZW4olLb0CTPQMug7QNL0HSBp+k79+vXrt8zN20aMGBHvvfde0mVAq1Rf38lMyBsRsdNOO8Uaa6yxyuMqKiri/vvvb/T45557bvznP/9Z7vV8Ph+PPPJIo8eDlsbFB5A0fQdImr4DJE3fAZKW+ZB3VZ5//vnYd999o1OnTrFgwYJGnz9kyJD497//XYTKoGVw8QEkTd8BkqbvAEnTd4Ck1dd3StIopFi+ukFbZWVlbL/99rH99tvHYYcdtsLjr7vuunjmmWciImLo0KHx0UcfJVInAAAAAEChtPgbrzXGW2+9FRFf3iTtzTffjIiIOXPmrPD4zz77LK6++uro1q1bfP7557Fo0aIkygQAAAAAKJhWtV1DfcrKyqJfv34REbH//vvH3nvvHWeddVZERMyePTvmz5+fZnmQKF8jApKm7wBJ03eApOk7QNJa/Z68q1JeXh7l5eVN2q8XWgMXH0DS9B0gafoOkDR9B0hafX2nVW3XsCpLliyJJUuWpF0GAAAAAEDBtKobrwEAAAAAtDVCXgAAAACADFvpnrwAAAAAALRsVvICAAAAAGSYkBcAAAAAIMOEvAAAAAAAGSbkBQAAAADIMCEvAAAAAECGCXkBAAAAADJMyAsAAAAAkGFCXgAAAACADBPyAgAAAABkmJAXAAAAACDDhLwAAAAAABkm5AUAAAAAyDAhLwAAAABAhgl5AQAAAAAyTMgLAAAAAJBhQl4AAAAAgAwT8gIAAAAAZJiQFwAAAAAgw4S8AAAAAAAZJuQFAAAAAMgwIS8AAAAAQIYJeQEAAAAAMkzICwAAAACQYUJeAAAAAIAME/ICAAAAAGSYkBcAAAAAIMOEvAAAAAAAGSbkBQAAAADIMCEvAAAAAECGCXkBAAAAADJMyAsAAAAAkGFCXgAAAACADBPyAgAAAABkmJAXAAAAACDDhLwAAAAAABkm5AUAAAAAyDAhLwAAAABAhgl5AQAAAAAyrGxlb+ZyuXxShQDFl8/nc2nXsCr6DrQu+g6QNH0HSJq+AyStvr5jJS8AAAAAQIYJeQEAAAAAMkzICwAAAACQYUJeAAAAAIAME/ICAAAAAGSYkBcAAAAAIMOEvAAAAAAAGSbkBQAAAADIMCEvAAAAAECGCXkBAAAAADJMyAsAAAAAkGFCXgAAAACADBPyAgAAAABkmJAXAAAAACDDhLwAAAAAABkm5AUAAAAAyDAhLwAAAABAhgl5AQAAAAAyTMgLAAAAAJBhQl4AAAAAgAwT8gIAAAAAZJiQFwAAAAAgw4S8AAAAAAAZJuQFAAAAAMgwIS8AAAAAQIYJeQEAAAAAMkzICwAAAACQYUJeAAAAAIAME/ICAAAAAGSYkBcAAAAAIMOEvAAAAAAAGSbkBQAAAADIMCEvAAAAAECGCXkBAAAAADJMyAsAAAAAkGFCXgAAAACADBPyAgAAAABkmJAXAAAAACDDhLwAAAAAABkm5AVYhaeeeipmz5693KO0tHSF5wwePHiZY3/7298mWDEAAADQlgh5AVaha9eu0b179+UeuVxuhee89957cfjhh0f37t3jqaeeiosvvji5ggEAAIA2RcgL0ESVlZXLvbbWWmvFBx98EPl8PhYvXhwRET/5yU/iiiuuSLo8AAAAoI0oS7sAgKzq2LHjcq9NmDAhNttss4iIGDZs2EpX+wIAAAAUgpW8AE2Uz+fTLgEAAADASl6Ab2rXrl20a9eu7nlJSf2fh3Xs2DHmzZuXVFkAAAAA9bKSF+AbLrrooli4cGHdY+edd673uLlz5yZcGQAAAMDyrOQF+JrOnTtH165dG3z8WmutFRMmTKh7XlpaGn379l3uuAULFtSFwj169IjKysqoqqpqfsEAAABAm2clL8DX/PSnP43TTz+9wcd//vnnyzxfbbXV4sUXX6x7vPXWWzF69OgYOnRo3THnnntu7LTTTgWrGQAAAGjbrOQF+JpcLtes86dMmRIbb7xx3fPvfe978d3vfjcuv/zyutfOO++8Zs0BAAAA8HVW8gI0w7Bhw1b6/vTp0+PDDz9MqBoAAACgLbKSF+BrPvnkkxgzZkwMHDhwlcc++eSTcfDBB6/0mFdeeSVeeeWVQpXXaPvss0907NhxmdeeeOKJlKoBAAAAikHIC/A1jz/+eORyufjud78bu+66awwYMGC5Yx544IFYsmRJHH/88SlU2Dj77rtv9O7de5nXhLwAAADQugh5U3DKKadEeXl5RETU1tbGTTfdlHJFwNc99thj8dhjj8V9991Xb8g7cuTIqKysjHw+n0J1DfeTn/wkJkyYEBMmTEi7FAAAAKCIhLwJO+ecc+LSSy+tC3nz+Xx07949Lr300pQrA77p0UcfjW233bZu64Ybb7wxZs2aFRUVFVFRURG5XK5FB72/+MUvYtNNN13u9RtuuCGFagAAAIBiEfIm5Nxzz42+ffvGySefXBfwRkTkcrn41a9+FT179oz58+fHRRddlGKVwNc99thjcdxxx9WFvF27do3a2to4//zzo7KyMuXqVuynP/1pbLTRRtG3b9+0SwEAAAASIOQtkmOPPTZ22223uueHHHJIdOvWrd5jS0tL42c/+1lMmzZNyAstzPXXXx8bbrhhbLTRRvHhhx/G1KlTo6amJu2yVmrcuHFx1llnRc+ePdMuBQAAAEhAbmVfNc7lci33e8gt1A9/+MM44ogjYuutt4511123UedOmzYtVl999SJVBhH5fD6Xdg2r0hL7zs477xyrrbZaPP/887Fw4cK0y2mQ119/Pbbffvt638vlWvwfA1oRfQdImr4DJE3fAZJWX9+xkrfANt544/je976XdhlAAb3yyitpl9Bop556ajz66KON/rAJAAAAyB4hL0ArNGLEiDjwwAOjffv2aZcCAAAAFJmQt4VYsGBB7LjjjmmXAbQio0aNSrsEAAAAIAH25C2gY445Jm644Ybo2rVro8/N5/Px3nvvxVZbbVWEyuBL9opqW1588cUYOHDgMq+tu+66UVtbm1JFtEX6DpA0fQdImr4DJM2evAU0YMCA+Ne//rXMa+3bt4+OHTs2abxcLhf9+vUrRGkA8fe//z122223KC0tXeb16dOnR69evVKqCgAAACgGIW8jde/ePb744osoLS2NDh06pF0OQJ1bbrkljjrqqIiI6NixY5SUlCx3TM+ePWP+/PmxZMkSYS8AAAC0EssnAG3YG2+8scxXm+fMmRNlZWVRVVUVZWVlUV1dHdOnT4/OnTsXNOBdvHhxtGvXLtZcc82CjQm0PRUVFdG5c+fo3LlzvQHvVzp37hydOnVKsDIAAACgmKzk/ZqysrL46KOP6p7ncrlYsmTJMv9bDPl8PpYuXVqUsYG2Y2V7rAMAAACtl5W835DL5eoeXz3/+v8W2uLFi237ABTEySefHI8++miDj//mfr1ZUVZWlnjtJSUlUVZWVvco1r8JAAAA0BRC3pRUVVVFZWVltG/fPu1SgFbi5ptvjm9/+9tRW1u7ymMrKipi4sSJCVRVWD179ozq6uoYMWJEovOee+65UV1dHdXV1bFo0aLYa6+9Ep0fAAAAVkbIm7C5c+fGnDlzomvXrtGxY8e0ywFakaFDh0aHDh3ixRdfXOWxS5YsiY022iiBqgpv0qRJscsuuyQ2X3l5+TL9+ic/+Um88MILic0PAAAAqyLkTcj06dNj8uTJMWDAgOjRo0dUV1enXRLQhpWXl8dbb72VdhlN0q9fv0RrP/744+OCCy5IbD4AAABoLDdeS8CECRNijz32iLFjx6ZdCtDK9e3bt0H7fC9evDhzK3lzuVwMGDAgsfk6duwY/fr1iz59+iQ2JwAAADSFkLfIxo0bF4cccoiAF0jEmWeeGYMHD17lcSUlJTFo0KAYNWpU8YsqkO222y5ef/31xObbbbfd4plnnlnmtS+++CLmzJmTWA0AAADQELZrKKLRo0fHf/3Xf2UqRAGy7eyzz4477rgj5s+fv9LjysrK4qqrrkqoquYrLS2Nq6++OrX5P//883j55Zdj6NCh8fzzz6dWBwAAANRHyPs1//znP+Ppp5+OqqqqZo/1/vvvx0knnRRvv/12ASoDaLgzzzwz7rjjjliwYMEKj3n88cfj29/+doJVNU9NTU3svffeiQas06dPj3fffTc+++yzuOCCC2K33XaLp59+OrH5AQAAoKFy+Xx+xW/mcit+sxW74YYbomfPnhER8eMf/zhyuVyDzx01alS8++678cc//jGGDx9erBKhSfL5fMP/MKekrfadYrjiiitizTXXXO712traOProo1OoqPk6deoUt912W0yePDnOOeecos+34447xsCBA+Puu+8u+lytlb4DJE3fAZKm7wBJq6/vCHlX4cYbb4zTTjstfv/738cpp5yy0mNHjhwZ559/vpVetFguPoCk6TtA0vQdIGn6DpC0+vqO7RpW4fTTT498Ph9TpkxZ5bHDhw8X8AIAAAAAibKStwFqa2tXuWXDiBEj4pxzzokXX3wxoaqg8XzCDCRN3wGSpu8ASdN3gKRZydsEt99+e4P25B09erSAFwAAAABInJB3FY477ri0SwAAAAAAWCEh70o8+eSTUVpamnYZAAAAAAArJORdib322qtBWzUAAAAAAOnZa6+94vXXX49TTz017VJSIeRdgddeey06dOiQdhkAAAAAwCr06tUrtt9++1hnnXXSLiUVQt4V2GijjRq8ivfFF1+MM844o8gVAQAAAAD1efrpp+MXv/hF2mWkJpfP51f8Zi634jdbuZkzZ0bPnj0bdOySJUvizjvvjKFDhxa5KmiefD7f4vcfact9B1ojfQdImr4DJE3fgZajffv2UVpaGgsXLky7lKKqr++UpVFIa1NeXh5dunRJuwwAAAAAaLOqqqrSLiE1tmsokCOOOCKuv/76tMsAAAAAANoYIW+BlJSURGlpadplAAAAAABtjJB3BWpqahp1fD6fj9ra2iJVAwAAAABQPzdeW4lZs2ZFx44dG3TsAw88EMcdd1yRK4LmcUMAIGn6DpA0fQdImr4DJK2+viPkbYSuXbtGPp+P+fPnp10KNImLDyBp+g6QNH0HSJq+AyStvr5ju4YGKisri7lz58YXX3wRvXv3jtLS0ujZs2faZQEAAAAAbZyQt5G6desW7733Xqy99tpxzz33pF0OAAAAANDG2a6hgcrKymLJkiXxwQcfxLRp02KvvfZKuyRoNF8jApKm7wBJ03eApOk7QNLsydsMZWVlMW/evAbfiA1aIhcfQNL0HSBp+g6QNH0HSJo9eZshn8/HP/7xj7TLAAAAAABYhpW80Ib4hBlImr4DJE3fAZKm7wBJs5IXAAAAAKCVEfICAAAAAGSYkBcAAAAAIMOEvAAAAAAAGSbkBQAAAADIMCEvAAAAAECGCXkBAAAAADJMyAsAAAAAkGFCXgAAAACADBPyAgAAAABkmJAXAAAAACDDhLwAAAAAABkm5AUAAAAAyDAhLwAAAABAhgl5AQAAAAAyTMgLAAAAAJBhQl4AAAAAgAwT8gIAAAAAZJiQFwAAAAAgw4S8AAAAAAAZJuQFAAAAAMgwIS8AAAAAQIYJeQEAAAAAMkzICwAAAACQYUJeAAAAAIAME/ICAAAAAGSYkBcAAAAAIMOEvAAAAAAAGSbkBQAAAADIMCEvAAAAAECGCXkBAAAAADJMyAsAAAAAkGFCXgAAAACADBPyNkKHDh1i4cKFsXDhwnj//ffTLgcAAAAAQMjbUGVlZTFv3rzo2LFjdOzYMTbffPOorq6OF198Me3SAAAAAIA2TMjbADU1NbFkyZIoKytb5vWysrLYc889o7a2Nu69996UqgMAAAAA2jIhbwPU1NRELpdb4fu5XC5KS0ujpMR/TgAAAAAgWVLJBigvL4/KysqVHnPEEUfErbfeGu3atUuoKgAAAAAAIW+Dde3adZXHnHzyyXH55ZdH+/btE6gIAAAAAEDI22D5fD4mTJgQkydPXulxZ555Zpx66qkJVQUAAAAAtHVC3gaqqamJtddeOzbeeOMYN25c2uUAAAAAAESEkLfR5s2bF9tvv32MGjUq7VIAAAAAAIS8TTFjxozYf//94+23317uvc8++yy++OKLFKoCAAAAANoiIW8TTZw4MU488cTlXr/vvvviL3/5SwoVAQAAAABtkZC3GWbNmhV///vf0y4DAAAAAGjDhLzN8Pnnn8epp54aTz/9dNqlAAAAAABtlJC3mcaNGxdnnnlmPPbYY2mXAgAAAAC0QULeAhgzZkw8+uijaZcBAAAAALRBQl4AAAAAgAzL5fP5Fb+Zy634TZax9tprx5AhQ+Ljjz+O0aNHp10O1Cufz+fSrmFV9B1oXfQdIGn6DpA0fQdIWn19R8gLbYiLDyBp+g6QNH0HSJq+AyStvr5juwYAAAAAgAwT8gIAAAAAZJiQFwAAAAAgw4S8AAAAAAAZJuQFAAAAAMgwIS8AAAAAQIYJeQEAAAAAMkzIC9AClZaWRkVFRSxatCh69+4d+Xw+8vl8vPPOO1FSonUDLUNJSUnkcrl4/vnn6/pUPp+Pvn37pl0aAAC0KZICgBakffv20b59+6iuro6qqqro0KFDTJ8+ve79wYMHx/Dhw6Ndu3YpVgnwpTvuuCP+67/+K0pLS5d5ffLkybHaaqulVBUAALQ9Ql6AFqJr164xc+bMqKysjFwut8Ljdt5553jyySejY8eOCVYHUL8HHngg9txzz+VenzZtWvTp0yeFigAAoO0R8gKkrHfv3tG3b9/47LPPGhzc7r///nHvvfdG165di1wdQP26dOkSHTp0WOkxU6dOjX79+tm+AQAAiiyXz+dX/GYut+I3gczJ5/MrXh7aQrTFvvPRRx/FRhtt1KRz77vvvvjZz34Ws2bNKnBVfKVr1671fu184sSJUVVVlUJF2aLvtF6/+93v4swzz2zQsTU1NVFWVlbkiuBL+g6QNH0HSFp9fcfVNkCKNthgg2jfvn2Tzz/yyCOjqqoqbr755pgwYYKwt0A6deoU66+/fkREHHrooXHBBRcsd8zRRx8d7777brPmmT59ekyePLlZY0AW5HK5GDRoUIwaNSrtUgAAoFWykhfaEJ8wtzxvv/12bLPNNgUZ64QTTog777yzIGO1ZZ06dYpjjz02brrppqLP9cADD8Tvf//7GDduXKsNe/Wd1mnttdeOq6++On70ox81+JzFixc360MtaCh9B0iavgMN16NHj+jTp098/PHHaZeSafX1HXvyAqRgm222iQMOOCC6detWsDG32GKLOOCAA6J3794FG7MtqaioiN133z023HDDRALeiIgjjjgi/vnPf8YFF1wQBxxwQBxwwAHRo0ePROaG5thpp51i2223bdQ5JSUlse+++xapIgAAWroePXrE6aefHrfddltsttlmaZfT+uTz+RU+IiLv4eHReh4r+/veUh5p/zdK6vH444/ni2WfffZJ/feXxccaa6yRHzt2bH7w4MFF+/+mIX7961/nf/zjH+c7d+6c+n+TQjzyLaCvrOqR9n+jLD4GDRqU/9vf/tboP9+LFi3K//jHP84ffPDBqf8ePFrvI+2eou94eLS9R9o9Rd/xyMKjR48e+V//+td114W33npr6jVl+VHf33N78gIkaKeddootttgi+vfvX7Q5DjrooHj//fdj6tSpRZujNVq4cGH8+c9/TruMuPDCCyMi4le/+lVcffXVUVlZmXJFsLxDDjkkvv3tbzf6vA4dOsT9998fM2fOjH79+sWUKVPiscceq3t/8ODBERHN3u8aAICWpVOnTrH55punXUarZrsGgAQdeuihceutt8aWW25ZtDlOO+20uOSSS2zb0Ei1tbVRVVUVxx9/fNqlRETEJZdcEhdddFFUVFSkXQoUXK9eveLWW2+Ns846a5nXu3btWtBtbAAAaBkmTJgQ11xzTUREjBo1Kp577rmUK2p9rOQtoB133DE6d+4czz//fNqlAN9w5ZVXrvC9Tz75JP74xz/GkCFD4tBDD23UuPl8Ps4777wGHbvPPvvEzjvv3Kjxm+rkk0+Odu3axYwZM+LXv/51LFy4MJF5s6isrCx++9vfRufOnWPo0KFpl7OM8847Lzp27BhnnnlmLF26NO1yoM4LL7wQ++yzT+y6664FHffll18u6HgAALQc48ePj3PPPTdGjx4dTz/9dNrltDq5/29vlvrfdPfFRtl0002jQ4cO8e9//zvtUqBe+TZ019drrrkmunTp8tWYccIJJ6zw2AkTJsSzzz4bG2+8ceyyyy6NnusPf/hD3a9nz54d55577nLH7L333nHllVfG1ltv3ejxm2u11VaLGTNmJD5vVrRv377Fb4nwpz/9KU444YSora1Nu5RGa0t9p6353e9+F2eeeWazxpg6dWo8+eSTy7z22GOPxbPPPtuscWnb9B0gafoOkLT6+o6QtwC22WabOOeccyIi4pFHHom//OUvKVcE9WsLFx9XX311rLPOOnHwwQen8jX3ysrKeOqpp5Z7feDAgXV7TSZNyLtypaWlcdRRR8Vdd92Vdikr9b//+79x2GGHpV1Go7WFvtNWbbnllnHdddfFHnvsUdBxR44cGT//+c/jhRdeKOi4tB36DpA0fQdIWn19x3YNzTRo0KD4wx/+EFtttVVERIwZM0bICyn47W9/G0OGDKnbNiUtHTp0yGQQ15bV1NTEfffdF5WVlfHggw+mXc4KHXroofHcc8/F/vvvn3YpEBER7733XvznP/8peMi7+eabx/rrry/kBQCARhDyNsPAgQPjwQcfjEGDBtW9dswxx8To0aNbdFAArck555wThxxySGyyySbRvXv3tMsho5YuXRrDhw9Pu4xV2m+//eLll18u+D6o0BI88cQTMWnSpMjlcvH444+nXQ4AAGRKmw55zzvvvPjwww/jiSeeaPS566yzTjzzzDOx/vrrL/P6WmutFf369StUidDmjBw5su7Xs2bNit12222Z9w844IC46qqr6p6vueaa0aNHj8Tqy5p99903Zs+enXYZFNAuu+wSb731Vmy77bZpl0Ibd9ZZZ8UPfvCDgoz17LPPxtChQ+PII4+MkpKSmDp1akHGBQCAtqJNhrzHHntsXHTRRdGzZ89YsmRJXH/99RERcdBBB8WoUaMaNEZ5eflyAe9Xzj///Pjiiy9s2wBN8PWV8TU1NfHpp58u836nTp1itdVWS7qszLrrrrtiyy23jFmzZqVdSiZ89tlnccopp8QzzzyTdikrNWTIkBgxYkRBb+RXWloaY8eOjTlz5tRtQdRc48aNK8g4tEx9+/Yt2Dco5s6dG5MmTSrIWAAA0Ba1yZC3W7du0b9//7rnvXv3joho8E2aVl999XjzzTdX+H7Pnj2jS5cuzaoR+DJ0+vrfVRpnl112iY8//thK3gaaOnVqbL311rHOOuukXcoq5XK5GDx4cEGC3kmTJkVZ2ZeXA6uttlrk8/mYNm1aRES8//77sffeezdqvA8//DB69epVNx6t0xlnnBFDhw4tyFjDhg2Lk08+OSIibrrppoKMCQAAbU1J2gUk6cADD4x58+bFFVdcsczrRxxxRHTt2jXeeeedVY7RrVu3GDt2bPTs2XOlx918881x8MEHN6tegOaYNWtWTJ8+PfJ5N9JtiNra2kyteP4q6H377bcbdd6IESNi7ty5MW/evJg3b16sscYasdpqq9UFsrlcru75HnvsEfPmzas7fu7cufHLX/5yuTH/7//+r+79jTbaaJnxaJ06deoUHTp0aNYYo0ePjq5du9Zdn0VELFq0KBYtWlSIEgEASFH79u3j888/T7uMNqVNreRt165dvSts77333qitra17PmDAgJg4ceJyx3Xo0CGmTZsW5eXlq5yrffv20a5du+YVDAArkcvlYuutt47FixdHRMRjjz0W//Vf/1XvsX//+99j9913j3bt2kUul2vQ+CUlJcv9u3nJJZfERRddtMxrjRmTbDv88MPj7rvvjtLS0maNM378+Bg8eHBUV1cXqDIAAFqSqqqq2GCDDdIuo01pUyt5V6SsrCzKy8vrHp9//nnU1NQs91iwYEGDAt6v/OUvf2n011wBCmXkyJFRU1NTtyUNrVMul6v79+tHP/pRvf9+1dTUxL777hvl5eXNDmNLS0uX+TezEGOSDQceeGD8+c9/jvLy8iaFvIsXL47S0tIoLS2N9ddfX8DbAFdcccVyf5c322yztMsCAGiQJUuWpF1CmyLkrUdJSckKH42Ry+X84AukpqSkJDbccMOYMWNG2qW0aN/s7e+++27svvvu6RRTAIX49wu+adddd42nnnpqpdc1tbW19T5qamoil8tF+/btl3mdVcvlcsv9XR41atQKb/4LAEDb5ac+gFZs3Lhxseaaa6ZdRotVWlq63D68W2yxRTz//POxdOnSlKqC7Kmuro4ddtihbqXu1x9f3diPwhk7dqzeDgDAMtpMyFtaWhrt27dPuwyAxE2YMCH69etX757kbV1NTU107959mdfef//9qKioiO9+97vpFAUtUE1NTcyfPz+qqqqWeb2qqirmzZsX++23X7z11lspVdc2TZgwIbp27Zp2GQAAtBBtIuQtKyuL7373u/HAAw+kXQpAKiZOnBjjxo3ztf1GqK6ujvnz56ddBrQIr776anTt2jWOPPLImD59et3jhBNOiG7dusWwYcPSLrHV6dixY3Ts2DHtMgAAyIhcPp9f8Zu53IrfTEnfvn3rXZE7fvz4ZZ6XlJTEOuusExFffvX2iSeeSKK85ey7777xj3/8I5W54Zvy+XwWNolucX0nS6ZNmxaLFi2q972BAwe60VEjHXzwwXH99ddHRESPHj2iW7du6RaUTS2+77TE6x3ati5dusRFF10UZ5111kqP23zzzWPBggXLXQe3dVm43tF3oHXRd4Ck1dd3MhfyDh8+PHbdddflXt9yyy3j/fffr3ves2fPmDlzZpKlLWPSpEkxa9asOPHEE+ONN95IrQ74uixcfISQt0mmTp0a06dPj5///Oc+WCqSM844I4477rhYZ511fEW6cVp832mJ1zu0bT/60Y/ixhtvjD59+qzy2NGjR8fmm2/uZnZfk4XrHX0HWhd9B0hafX2n1Xxv97333ovtt98+Ir5cxfutb30r1Xouuuii2HzzzQW8QNFNmDAhzjjjjNh8880FvEV03XXXxeabbx433nhjzJ07N+1ygFbs4YcfrvsWwapstdVWAl4AAKJV3e74tddei5KSkujQoUM899xzqdUxZsyYmDhxYmrzA23D5MmTY+TIkfHAAw/Egw8+mHY5bcaFF14YpaWlMXToUNs3AEWx9tprx8CBA9MuAwCADGlVIW9ExGGHHVbvnr1JGT16dJx77rmphsxA6zZ9+vQYNmxY/POf/4ybb7457XLapPPPPz9qamrqQpiDDz44KioqUq4KaC0OPPDAOOaYY9IuAwCADGlVIW8ul4uHH3441RoeffTRePrpp1OtAWhdlixZEvfcc0/d8zFjxsTvfve7FCsi4ssVvV+55ppr4rTTTot27dqlWBEAAABtVasKedM0evToePrpp+Of//xn2qVApv3ud79b5d3E24p8Ph9XX311VFZWxsUXX5x2OazEmWeeGSeeeKKQF0jcmWeeGVdccUWs7GbKAAC0frmVXRC2tLsvHnnkkfGb3/wm1llnnbRLWc79998fRx55ZNplwEpl4a6vZWVl+aVLl6ZdRouwdOlSoWGGzJs3L373u99FdXV1lJaWxqWXXrrM+9OmTYvHHnssTj755JQqTE2L7zst7XoHTjnllLj11lsbfHxpaambr31NFq539B1oXfQdIGn19Z1MreQ97LDDWlzAO3bs2Lj22mtjzJgxaZcCrUJtbW2cfvrpceONN6ZdSqqGDh3qB/aMOeOMM+JPf/pT1NbWRi6Xi5kzZy7z/qxZs+Kll16K9957L7bZZps4/vjj4x//+Ec89thjyxz385//PDbYYIMkSwcy7pZbbona2to49dRT0y4FAICUZCrkbWk+++yzOOWUU+KFF15IuxRoNfL5fNxyyy1RWVkZf/jDH9IuJzW33Xabr95mzJ133ln363w+H7///e/rPe6f//xnHHHEERERseaaa8aOO+4Yd999d7z44osREXH44YcLeYFG+elPf1rXVwAAaJsyFfJecskl0b9//xg0aFAq8y9ZsiQOOeSQuufz58+PV155JZVaoDWrra2Nu+++OyorK+P+++9Pu5xEff/734+qqioBbys2ZcqUePXVV2PhwoVxww03RETEJ598Uvf+OeecEw888ECst956aZUIpGj33XePoUOHNuqcgw46yI1/AQDauEztyRsRMXz48Nh1110Tn7empiZ22223ePXVVxOfGwola3tFtWvXLrbbbrvYYIMN4k9/+lOaZRXd97///Zg+fXq89tprtmloA3r37h3du3ePsWPH1vv+1ltvHR07doyIiGeffTY6d+6cZHkFs/vuu8dLL72Uqb4Dadp+++3j7rvvjo022qjB5+y5554xbNiwIlaVPVm73gGyT98Bkpb5PXnTtPXWW8f777+fdhnQplRXV8crr7wSb7/9dkRE4kHv+eefHwcccED86U9/in//+98REfHiiy9G7969Cz7XW2+9FRMnTiz4uLRMM2bMiBkzZqzw/REjRtT9Oqs3IhwyZEjd3xugYXr06NGogHennXaK119/vYgVAQCQFULeBhLwQnqqqqpWuOKxUJ599tk47bTTlnlt6tSpcffdd8fs2bOjqqoqIiK+9a1vxXvvvRddu3aNiIgNN9ywUVsrXHjhhXH00UfHKaecEv/4xz/qXp8yZUoBfhe0RltttVV89NFHUVFRkXYpDfKtb30rZs+eHePGjUu7FMiUIUOGLLO3d0OMGjXK9j4AAESE7RoapF+/fjF58uRE54RiyPLXiNq1axe9evWKiIijjjoqrrzyylWOteGGG8aCBQsiImLixImx5pprrvDYqqqqmDNnToNq7NOnT5SUlERE48PZLl26RKdOnWL27NmxePHiRp1L21VZWRkbbLBBTJgwIe1S6vXtb3873nnnnYj48sORr19bZLnvQFI222yzePnll6Nnz56NOq9bt24xb968IlWVXfoOkDR9B0hafX1HyNsAJSUlVknQKrSWi4/y8vK6/UpX5uuhbbdu3WLu3LnNqg3S8tWf35b2b9FRRx0VTz31VMyfPz9qamrqPaa19B0opsGDB9d9UNIYQt766TtA0vQdIGmtIuQtLy+Pf/3rX7HFFlskNqeQl9bCxQdkW/v27Zd5vtZaa8Unn3yS2Py33nprnHnmmXXPlyxZssobBeo7tHVffPHFMnu5L126NLp06bLMMblcLvbcc8944YUXGjTmJptsEuPHj6/bSohl6TtA0vQdIGmt4sZrzz33XKIBLwC0FN8MdMaOHRuDBg2KUaNGJTJ/TU2NUAkaqUOHDst9QPPVDRUnT54ca6+9dmyxxRbx/PPPN3jMqqoqfxcBAFhG5lbyRnx51/Gtttoq8vl85HLF+8Csc+fOsXDhwqKND0nzCTO0bt/73vfikUceqXte37+RX/3b+dW//xUVFVFZWVm3z/TXjRgxIoYMGdKsmvQd2qoPP/wwNt5444KO+dXf2wEDBsT48eMLOnZrou8ASdN3gKTV13eW/4kuA7beeuv48MMPo1+/fjF9+vSIiKiurq57FEJ1dbUtGgDIlMceeyxKSkrqHtdff33ddgpLly6N6urqGDhwYHz22WfRq1evKCkpierq6igrK1vmvK8ezQ14gearqampu8bdcccdo6SkRMALAMByMhnyRkRsuummMWXKlOjTp0/MmDEjOnToEOXl5dGnT5+CjN+3b99YtGhRQcYCgDScccYZ8cc//jEWLFgQe+21V5SXl8fYsWOjf//+MXv27LTLA1ZhyZIlccIJJ0R5eXmUl5fHG2+8kXZJAAC0UJncruHrevToUfcV01mzZq3yBjAN1atXr5g1a1ZBxoKWwteIgKTpO7RVr7/+emy44YbRs2fPRm8vtnjx4liwYEFccsklcdNNNxWpwtZL3wGSpu8ASauv72Q+5B05cmR07949IiLWXXfdqKmpafaYkydPjk033TTmzJnT7LGgJXHxASRN36Gte//996NHjx6NOufee++NX/7yl0WqqPXTd4Ck6TtA0lplyPt1uVwuRo8eHSUlJTFw4MAmj7PWWmvFxIkTC1gZtAwuPoCk6TtA0vQdIGn6DpC0+vpOWRqFFEs+n49NNtkkOnXqFAsWLGjSGKNGjSrYzdsAAAAAAIqtVYW8ERF77LFHtG/fvsnn//CHP4xp06YVsCIAAAAAgOJpVSFvLpeL0047LcrKWtVvCwAAAABghUrSLqAQjjzyyMjlcnH00UfHk08+GX/729/SLgkAAAAAIBGZX/J6yimnxA033BC9e/eOa6+9Nu1yAAAAAAASlcvnV3yDxSzcffGcc86JDh06LPNaWVlZXHDBBY0e64477ojzzz8/Zs6cWajyoEVx11cgafoOkDR9B0iavgMkrb6+k/mQtz6dOnWKBQsWNPq8IUOGxL///e8iVAQtg4sPIGn6DpA0fQdImr4DJK2+vpP57Rrqs3jx4jjmmGOiZ8+eDdrC4cYbb4wRI0bE+PHji18cAAAAAEABtcqVvF9Za6214osvvljlcYccckg88cQTCVQE6fIJM5A0fQdImr4DJE3fAZJWX98pSaOQpEyfPj2OPvrolR5z5ZVXxquvvppQRQAAAAAAhdWqV/JGRFRUVMSgQYMiImKPPfaIq6++OiK+vMnaHXfcEePHj3ejNdoMnzADSdN3gKTpO0DS9B0gaW1mT96vW7x4cd3N1NZZZ52IiLjnnnviF7/4RcyaNSvN0gAAAAAAmq3Vh7zf9Je//CX+53/+J+bOnZt2KQAAAAAAzdbqt2v4unbt2kW7du1i0aJFaZcCqfA1IiBp+g6QNH0HSJq+AyStTW7X8HXV1dVRXV2ddhkAAAAAAAVTknYBAAAAAAA0nZAXAAAAACDDhLwAAAAAABkm5AUAAAAAyDAhLwAAAABAhgl5AQAAAAAyTMgLAAAAAJBhQl4AAAAAgAwT8gIAAAAAZJiQFwAAAAAgw4S8AAAAAAAZJuQFAAAAAMgwIS8AAAAAQIYJeQEAAAAAMkzICwAAAACQYUJeAAAAAIAME/ICAAAAAGSYkBcAAAAAIMOEvAAAAAAAGSbkBQAAAADIMCEvAAAAAECGCXkBAAAAADJMyAsAAAAAkGFCXgAAAACADMvl8/m0awAAAAAAoIms5AUAAAAAyDAhLwAAAABAhgl5AQAAAAAyTMgLAAAAAJBhQl4AAAAAgAwT8gIAAAAAZNj/Axo4YAmQAO+MAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x576 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Path to first batch image file\n",
    "# shadow mask folder\n",
    "mask_path_train = 'DESOBA_DATASET/Mfs_train'\n",
    "\n",
    "\n",
    "# Get a list of all image file names in the folder\n",
    "image_files = os.listdir(mask_path_train)\n",
    "\n",
    "# Display the first 10 images\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 8))\n",
    "fig.tight_layout()\n",
    "\n",
    "for i, image_file in enumerate(image_files[:10]):\n",
    "    # Open the image using PIL\n",
    "    image_path = os.path.join(mask_path_train, image_file)\n",
    "    image = Image.open(image_path)\n",
    "    # Display the image\n",
    "    axs[i // 5, i % 5].imshow(image, cmap='gray')\n",
    "    axs[i // 5, i % 5].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb04ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
